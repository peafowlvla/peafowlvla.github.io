<!DOCTYPE html>
<html>
<head>
  <meta name="google-site-verification" content="Io5PBEGCOEzWaL7oGOG-6-qHqYLZsvb7vAXvdoEF-1Y" />
  <meta charset="utf-8">
  <meta name="description"
        content="PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation">
  <meta name="keywords" content="Spatial Perception, Vision-Language-Action, Robotic Manipulation, 3D Vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- 请替换为你自己的 Google Analytics ID，如果没有可以注释掉 -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="icon" type="image/svg+xml"
  href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'%3E%3Ctext y='.9em' font-size='90'%3E%F0%9F%A6%9A%3C/text%3E%3C/svg%3E">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Qingyu Fan</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="">Zhaoxiang Li</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://yeelou.github.io/">Yi Lu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Wang Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://shenqiu.njucite.cn/">Qiu Shen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.xxlong.site/">Xiao-xiao Long</a><sup>2&#8224;</sup>
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/caiyinghao">Yinghao Cai</a><sup>1&#8224;</sup>
            </span>
            <span class="author-block">
              <a href="">Tao Lu</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XL9j2UUAAAAJ&hl=zh-CN">Shuo Wang</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institute of Automation, Chinese Academy of Sciences</span><br>
            <span class="author-block"><sup>2</sup>Nanjing University</span>
          </div>
          
          <div class="is-size-6 publication-authors">
            <span class="author-block"> <sup>&#8224;</sup>Corresponding authors.</span>
          </div>

          <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- 视频部分 -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation</h2>
        <div class="content has-text-justified">
          <div class="has-text-centered" style="position: relative;">
            <center>
                <iframe width="840" height="472.5"
                        src="https://www.youtube.com/embed/nDeWHDPLbT8?si=qrpwgaQNRCzPpAT5"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                        referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            </center>
          </div>
          <!-- 视频下方的说明文字 -->
          <div style="text-align: left; max-width: 800px; margin: 1rem auto;">
            <ul>
              <li><strong><em>From View-Agnostic Tokens to 3D-Consistent Fusion</em></strong>: Prior bimanual VLAs treat multi-view inputs as an orderless token set, yielding fragile spatial beliefs under occlusions and viewpoint shifts; PEAfowl enforces cross-view 3D alignment to produce consistent spatial tokens.</li>
              <li><strong><em>From Noisy Depth as a Liability to Depth as a Prior</em></strong>: Commodity depth often destabilizes learning and hurts transfer; PEAfowl distills geometry-aware priors into per-token depth predictions during training, improving real-sensor robustness with zero test-time overhead.</li>
              <li><strong><em>From Global Text Conditioning to Targeted Evidence Retrieval</em></strong>: Global language conditioning tends to be coarse and instruction-agnostic in clutter; PEAfowl uses text-as-query readout to retrieve and aggregate instruction-relevant visual evidence for sharper grounding.</li>
              <li><strong><em>From Unstable Generalization to Reliable Multi-Task Transfer</em></strong>: Existing bimanual VLAs degrade sharply under multi-task learning and scene variations; PEAfowl delivers consistent gains in simulation and real-robot success across diverse tasks.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- <script>
    // 控制视频声音按钮
    const video = document.getElementById('activevla-video');
    const btn = document.getElementById('unmute-btn');

    btn.addEventListener('click', () => {
      if (video.muted) {
        video.muted = false;
        btn.textContent = '关闭声音';
      } else {
        video.muted = true;
        btn.textContent = '开启声音';
      }
      video.play();
    });
  </script> -->
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- 使用 Flexbox 居中 -->
      <div class="has-text-centered">
        <!-- 请替换为 ActiveVLA 的 Figure 1 (teaser.png) -->
        <img src="./static/images/Fig2.png" alt="PEAfowl architecture" style="width: 80%; height: auto;">
      </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">PEAfowl architecture</span>. Traditional bimanual VLA systems often fail in cluttered, multi-object scenes because multi-view inputs are treated as view-agnostic token concatenations and language is injected only as global conditioning, leading to brittle 3D understanding and coarse, instruction-agnostic attention under occlusions and viewpoint shifts. In contrast, <strong>PEAfowl</strong> explicitly builds 3D-consistent multi-view representations via geometry-guided fusion (distributional depth, differentiable 3D lifting, and cross-view neighbor aggregation) and replaces global text conditioning with a text-as-query readout over frozen CLIP features, allowing the policy to retrieve instruction-relevant evidence and remain stable under scene variations. To further handle noisy and missing commodity depth in the real world, PEAfowl applies training-only depth distillation from a pretrained depth teacher—injecting geometry-aware priors with zero test-time overhead—resulting in more reliable bimanual manipulation and sim-to-real transfer even under challenging tasks.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p align="center">
            <img src="./static/images/Fig1.png" alt="" style="width:70%"></img>
          </p>
          <p>
            Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding. In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors. On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">PEAfowl Architecture</h2>
        <div class="content has-text-justified">
          <div class="has-text-centered">
            <img src="./static/images/Fig2.png" alt="PEAfowl architecture" style="width: 100%; height: auto;">
          </div>
          <p>
            ActiveVLA adopts a two-stage, <strong>coarse-to-fine</strong> strategy. In the <strong>coarse stage</strong>, three orthographic projections of the 3D scene and a language instruction are processed by the VLM backbone (PaliGemma) to generate 2D heatmaps, which are then back-projected to locate the most relevant 3D region. 
          </p>
          <p>
            In the <strong>fine stage</strong>, an <strong>Active Perception</strong> module takes over. It first performs <strong>Active Viewpoint Selection</strong> to choose new camera views that maximize visibility and diversity based on the identified region. Then, it executes an <strong>Active 3D Zoom-in</strong> strategy to render high-resolution details of the critical area. The refined visual inputs are fed back into the VLM to predict heatmaps for key end-effector positions, while an action decoder outputs the final 3D action (rotation, gripper state, etc.). This closed-loop design allows the robot to "look closer" before acting.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

  
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment Setup</h2>
        <div class="content has-text-justified">
          <div class="columns">
            <img src="./static/images/Fig4.png" alt="Setup" style="width: 100%; height: auto;">
          </div>
          <p>
            (a) RoboTwin 2.0 simulation (Aloha-AgileX, 4-camera RGB-D) under Clean (top) and Domain-Randomized (bottom) settings. (b) Dual-arm AgileX Piper with a 4-camera rig.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Figure 3: Qualitative Results (Simulation) -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
        <h2 class="title is-4">Cross-view Token Consistency</h2>
        <div class="content has-text-justified">
          <p>
            we flatten the multi-view 2D tokens at the same feature level into a set of points, apply t-SNE to reduce the token embeddings to 3D, and then min–max normalize each dimension and map the resulting 3D coordinates directly to RGB channels. This produces a pseudo-colored “token image” for each view, which we use to inspect cross-view consistency.
          </p>
          <div class="has-text-centered">
            <img src="./static/images/Fig5.png" alt="Cross-view Token Consistency" style="width: 100%; height: auto;">
          </div>
          <p>
            SEM exhibits stripe-like color patterns that correlate more with image coordinates, with weak color correspondence for the same physical regions across views. PEAfowl (Before Aggregation) already reveals clearer object structures, but still shows cross-view drift. After Cross-View Aggregation, the same objects and regions become more color- and boundary-consistent across views, indicating stronger cross-view, 3D-consistent token alignment.
          </p>
        </div>
        <h2 class="title is-4">Depth-Distribution Predictions</h2>
        <div class="content has-text-justified">
          <p>
            We visualize the predicted discrete depth distribution per token by first normalizing it into a probability distribution, then computing the expected depth map
            <code>E[d] = Σ_d p(d) · bin(d)</code>
            using the depth bins, and finally normalizing the expected depth over the bin range and rendering it with a colormap. This allows a direct comparison of depth sharpness and completeness.
          </p>
          <div class="has-text-centered">
            <img src="./static/images/Fig6.png" alt="Depth-Distribution Predictions" style="width: 100%; height: auto;">
          </div>
          <p>
            SEM is more prone to blurred predictions and missing structures; PEAfowl w/o Depth Distillation already yields cleaner foreground contours; and full PEAfowl further improves boundary sharpness and region completeness, providing more stable geometric cues under noisy and incomplete commodity depth, which implicitly supports more reliable geometry-guided perception and downstream policy learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Figure 4: Real-World Results -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Real-World Experiments: Handling Severe Occlusions</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate ActiveVLA in real-world scenarios characterized by complex spatial structures and severe occlusions. This corresponds to <strong>Figure 4</strong> in the paper.
          </p>
          <div class="has-text-centered">
            <img src="./static/images/visualization_2.png" alt="Real-world manipulation tasks" style="width: 100%; height: auto;">
          </div>
          <p>
            The figure demonstrates ActiveVLA's capability in four challenging tasks:
          </p>
          <ul>
            <li><strong>Pick up the banana (Occluded):</strong> The banana is hidden by other fruits. ActiveVLA changes the viewpoint to find it.</li>
            <li><strong>Stack blocks (Occluded):</strong> The target red cube is hidden behind other blocks. The model finds a view to reveal it.</li>
            <li><strong>Cup on holder (Occluded):</strong> The cup is obscured by the holder structure.</li>
            <li><strong>Towel in drawer (Occluded):</strong> The towel is inside a drawer, requiring the robot to look inside.</li>
          </ul>
          <p>
            In all cases, ActiveVLA actively perceives the environment to resolve ambiguities and precisely completes the tasks, demonstrating strong generalization to real-world constraints.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>..
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
