<!DOCTYPE html>
<html>
<head>
  <meta name="google-site-verification" content="Io5PBEGCOEzWaL7oGOG-6-qHqYLZsvb7vAXvdoEF-1Y" />
  <meta charset="utf-8">
  <meta name="description"
        content="PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation">
  <meta name="keywords" content="Spatial Perception, Vision-Language-Action, Robotic Manipulation, 3D Vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- 请替换为你自己的 Google Analytics ID，如果没有可以注释掉 -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="icon" type="image/svg+xml"
  href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'%3E%3Ctext y='.9em' font-size='90'%3E%F0%9F%A6%9A%3C/text%3E%3C/svg%3E">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Qingyu Fan</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="">Zhaoxiang Li</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://yeelou.github.io/">Yi Lu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Wang Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://shenqiu.njucite.cn/">Qiu Shen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.xxlong.site/">Xiao-xiao Long</a><sup>2&#8224;</sup>
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/caiyinghao">Yinghao Cai</a><sup>1&#8224;</sup>
            </span>
            <span class="author-block">
              <a href="">Tao Lu</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XL9j2UUAAAAJ&hl=zh-CN">Shuo Wang</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institute of Automation, Chinese Academy of Sciences</span><br>
            <span class="author-block"><sup>2</sup>Nanjing University</span>
          </div>
          
          <div class="is-size-6 publication-authors">
            <span class="author-block"> <sup>&#8224;</sup>Corresponding authors.</span>
          </div>

          <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2601.08325v1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ZhenyangLiu/ActiveVLA-Injecting-Active-Perception-into-VLA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/ZhenyangLiu/ActiveVLA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- 视频部分 -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation</h2>
        <div class="content has-text-justified">
          <div class="has-text-centered" style="position: relative;">
            <center>
                <iframe width="840" height="472.5"
                        src="https://www.youtube.com/embed/nDeWHDPLbT8?si=qrpwgaQNRCzPpAT5"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                        referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            </center>
          </div>
          <!-- 视频下方的说明文字 -->
          <div style="text-align: left; max-width: 800px; margin: 1rem auto;">
            <ul>
              <li><strong><em>From View-Agnostic Tokens to 3D-Consistent Fusion</em></strong>: Prior bimanual VLAs treat multi-view inputs as an orderless token set, yielding fragile spatial beliefs under occlusions and viewpoint shifts; PEAfowl enforces cross-view 3D alignment to produce consistent spatial tokens.</li>
              <li><strong><em>From Noisy Depth as a Liability to Depth as a Prior</em></strong>: Commodity depth often destabilizes learning and hurts transfer; PEAfowl distills geometry-aware priors into per-token depth predictions during training, improving real-sensor robustness with zero test-time overhead.</li>
              <li><strong><em>From Global Text Conditioning to Targeted Evidence Retrieval</em></strong>: Global language conditioning tends to be coarse and instruction-agnostic in clutter; PEAfowl uses text-as-query readout to retrieve and aggregate instruction-relevant visual evidence for sharper grounding.</li>
              <li><strong><em>From Unstable Generalization to Reliable Multi-Task Transfer</em></strong>: Existing bimanual VLAs degrade sharply under multi-task learning and scene variations; PEAfowl delivers consistent gains in simulation and real-robot success across diverse tasks.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>

  <script>
    // 控制视频声音按钮
    const video = document.getElementById('activevla-video');
    const btn = document.getElementById('unmute-btn');

    btn.addEventListener('click', () => {
      if (video.muted) {
        video.muted = false;
        btn.textContent = '关闭声音';
      } else {
        video.muted = true;
        btn.textContent = '开启声音';
      }
      video.play();
    });
  </script>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- 使用 Flexbox 居中 -->
      <div class="has-text-centered">
        <!-- 请替换为 ActiveVLA 的 Figure 1 (teaser.png) -->
        <img src="./static/images/Fig1.png" alt="PEAfowl architecture" style="width: 80%; height: auto;">
      </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">PEAfowl architecture</span>. Traditional bimanual VLA systems often fail in cluttered, multi-object scenes because multi-view inputs are treated as view-agnostic token concatenations and language is injected only as global conditioning, leading to brittle 3D understanding and coarse, instruction-agnostic attention under occlusions and viewpoint shifts. In contrast, <strong>PEAfowl</strong> explicitly builds 3D-consistent multi-view representations via geometry-guided fusion (distributional depth, differentiable 3D lifting, and cross-view neighbor aggregation) and replaces global text conditioning with a text-as-query readout over frozen CLIP features, allowing the policy to retrieve instruction-relevant evidence and remain stable under scene variations. To further handle noisy and missing commodity depth in the real world, PEAfowl applies training-only depth distillation from a pretrained depth teacher—injecting geometry-aware priors with zero test-time overhead—resulting in more reliable bimanual manipulation and sim-to-real transfer even under challenging tasks.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding. In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors. On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Framework. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The Framework of ActiveVLA</h2>
        <div class="content has-text-justified">
          <!-- 图片居中，请替换为 ActiveVLA 的 Figure 2 (pipeline.png) -->
          <div class="has-text-centered">
            <img src="./static/images/pipeline.png" alt="ActiveVLA Pipeline" style="width: 100%; height: auto;">
          </div>
          <p>
            ActiveVLA adopts a two-stage, <strong>coarse-to-fine</strong> strategy. In the <strong>coarse stage</strong>, three orthographic projections of the 3D scene and a language instruction are processed by the VLM backbone (PaliGemma) to generate 2D heatmaps, which are then back-projected to locate the most relevant 3D region. 
          </p>
          <p>
            In the <strong>fine stage</strong>, an <strong>Active Perception</strong> module takes over. It first performs <strong>Active Viewpoint Selection</strong> to choose new camera views that maximize visibility and diversity based on the identified region. Then, it executes an <strong>Active 3D Zoom-in</strong> strategy to render high-resolution details of the critical area. The refined visual inputs are fed back into the VLM to predict heatmaps for key end-effector positions, while an action decoder outputs the final 3D action (rotation, gripper state, etc.). This closed-loop design allows the robot to "look closer" before acting.
          </p>
        </div>
      </div>
    </div>
    <!--/ Framework. -->
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Results -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <!-- 并排图片，请替换为 ActiveVLA 的实验结果图 (如 Figure 3 或 Figure 4) -->
          <div class="columns">
            <div class="column">
              <img src="./static/images/quantative_1.png" alt="RLBench Simulation Results" style="width: 100%; height: auto;">
              <p class="is-size-7 has-text-centered">Fine-grained manipulation in RLBench simulation.</p>
            </div>
            <div class="column">
              <img src="./static/images/quantative_2.png" alt="COLOSSEUM Simulation Results" style="width: 100%; height: auto;">
              <p class="is-size-7 has-text-centered">Fine-grained manipulation in COLOSSEUM simulation.</p>
            </div>
          </div>

          <ul class="content">
            <li><strong>Simulation Benchmarks:</strong> ActiveVLA achieves a new state-of-the-art on RLBench with a <strong>91.8%</strong> average success rate, surpassing previous methods like BridgeVLA and 3D Diffuser Actor. It also dominates on the COLOSSEUM benchmark (robustness to perturbations) and GemBench (generalization).</li>
            <li><strong>Real-World Robustness:</strong> As shown in the figures, ActiveVLA effectively handles complex real-world scenarios involving severe occlusions (e.g., retrieving a towel from layered drawers, grasping an occluded banana). By actively selecting viewpoints and zooming in, it can perceive fine-grained geometric details that static cameras miss.</li>
            <li><strong>Ablation Studies:</strong> Experiments confirm that both Active View Selection (A-VS) and Active 3D Zoom-in (A-3Z) are crucial. A-VS guides <em>where</em> to look, and A-3Z determines <em>how closely</em> to observe, forming a powerful hierarchical perception strategy.</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Results -->
  </div>
</section>

<!-- Figure 3: Qualitative Results (Simulation) -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results: Coarse-to-Fine Perception</h2>
        <div class="content has-text-justified">
          <p>
            We visualize the internal reasoning process of ActiveVLA on fine-grained manipulation tasks. This corresponds to <strong>Figure 3</strong> in the paper.
          </p>
          <!-- 请确保图片路径正确，对应 Figure 3 -->
          <div class="has-text-centered">
            <img src="./static/images/visualization_1.png" alt="Qualitative results of fine-grained manipulation" style="width: 100%; height: auto;">
          </div>
          <p>
            As shown above, the process is divided into two stages:
            <br>
            <strong>1. Coarse Stage (Left of dotted line):</strong> The model projects 3D modalities onto orthographic images (a) and predicts heatmaps to roughly mark critical regions (b).
            <br>
            <strong>2. Fine Stage (Right of dotted line):</strong> Based on these regions, ActiveVLA performs <strong>Active View Selection</strong> (c) to find the best angle and <strong>Active 3D Zoom-in</strong> (d) to capture high-resolution details. This enables precise manipulation in complex scenes like "Sweeping dirt to the dustpan" or "Placing jello in the cupboard".
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Figure 4: Real-World Results -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Real-World Experiments: Handling Severe Occlusions</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate ActiveVLA in real-world scenarios characterized by complex spatial structures and severe occlusions. This corresponds to <strong>Figure 4</strong> in the paper.
          </p>
          <!-- 请确保图片路径正确，对应 Figure 4 -->
          <div class="has-text-centered">
            <img src="./static/images/visualization_2.png" alt="Real-world manipulation tasks" style="width: 100%; height: auto;">
          </div>
          <p>
            The figure demonstrates ActiveVLA's capability in four challenging tasks:
          </p>
          <ul>
            <li><strong>Pick up the banana (Occluded):</strong> The banana is hidden by other fruits. ActiveVLA changes the viewpoint to find it.</li>
            <li><strong>Stack blocks (Occluded):</strong> The target red cube is hidden behind other blocks. The model finds a view to reveal it.</li>
            <li><strong>Cup on holder (Occluded):</strong> The cup is obscured by the holder structure.</li>
            <li><strong>Towel in drawer (Occluded):</strong> The towel is inside a drawer, requiring the robot to look inside.</li>
          </ul>
          <p>
            In all cases, ActiveVLA actively perceives the environment to resolve ambiguities and precisely completes the tasks, demonstrating strong generalization to real-world constraints.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Theme modified from 
            <a href="https://github.com/nerfies/nerfies.github.io" target="_blank" rel="noopener noreferrer">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
